{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Algorithms - Assignment 6 (30 points)\n",
    "Cognitive Algorithms        \n",
    "Summer Term 2019      \n",
    "Technische Universität Berlin     \n",
    "Fachgebiet Maschinelles Lernen \n",
    "\n",
    "**Due on July 1, 2019 23:55 via ISIS**\n",
    "\n",
    "**Answer the questions on Isis in 'Assignment 6 - Quiz' and copy code from this notebook where necessary**\n",
    "                                  \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 Linear Activation Function\n",
    "It can be shown, if a multilayer perceptron has a linear activation function in all neurons, any number of layers can be reduced to a two-layer input-output model.                                         \n",
    "Consider an MLP with $N$ input neurons $x_n$, one hidden layer with $K$ neurons $z_k$ and a single output $y$. ${\\alpha_k}_n$ define the weights connecting neuron $x_n$ and $z_k$, and $\\beta_k$ the weights connecting $z_k$ and the output. ${\\alpha_k}_0$ and $\\beta_0$ are the weights of the biases. All neurons have a linear activation function, thus the output of a hidden neuron can be written as \n",
    "$$z_k = -{\\alpha_k}_0 + \\sum_{n=1}^{N} {\\alpha_k}_n x_n$$\n",
    "and the total output becomes \n",
    "$$y = -\\beta_0 + \\sum_{k=1}^{K} \\beta_k z_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that there exists an MLP without hidden layers, which models the same function.\n",
    "\n",
    "Write your answer here in LaTex, then copy it to the answer field in the quiz. Make sure to use double $$ when writing Tex in ISIS.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer]**                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming \n",
    "---\n",
    "Like in the first assignment you aim to recognize handwritten digits. This time you will not train a linear perceptron, but a non-linear multilayer perceptron (MLP). You won’t have to implement it – we just want you to play around with existing code and modify it slightly. We are using the ```scikit-learn``` implementation, that can be found here:            \n",
    "http://scikit-learn.org/stable/modules/neural_networks_supervised.html            \n",
    "You might have to install ```scikit-learn``` beforehand. Follow the instructions on their webpage to do so.                   \n",
    "This time we will use the full MNIST Data set. Download the MNIST data set from ISIS if not done yet.                        \n",
    "\n",
    "Below you find the code to load the MNIST dataset and to train an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "import os.path\n",
    "import scipy as sp\n",
    "import pylab as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got MNIST with 52500 training- and 17500 test samples\n",
      "Digit distribution in whole dataset: [6903 7877 6990 7141 6824 6313 6876 7293 6825 6958]\n",
      "Training model.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "PATH = 'mlp_model.pkl'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import scipy.io\n",
    "    mnist = scipy.io.loadmat('mnist-original.mat')\n",
    "    X, y = mnist['data'], mnist['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.T/255.,y[0,:],test_size=0.25, random_state=0)\n",
    "\n",
    "    print('Got MNIST with %d training- and %d test samples' % (len(y_train), len(y_test)))\n",
    "    print('Digit distribution in whole dataset:', np.bincount(y[0,:].astype('int64')))\n",
    "\n",
    "    clf = None\n",
    "    if os.path.exists(PATH):\n",
    "        print('Loading model from file.')\n",
    "        clf = joblib.load(PATH).best_estimator_\n",
    "    else:\n",
    "        print('Training model.')\n",
    "        params = {'hidden_layer_sizes': [(256,), (512,), (128, 256, 128,)]}\n",
    "        mlp = MLPClassifier(verbose=10, learning_rate='adaptive', random_state=1)\n",
    "        clf = GridSearchCV(mlp, params, verbose=10, n_jobs=-1, cv=5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('Finished with grid search with best mean cross-validated score:', clf.best_score_)\n",
    "        print('Best params appeared to be', clf.best_params_)\n",
    "        joblib.dump(clf, PATH)\n",
    "        clf = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 + 8\n",
    "Run the code (this may take a while when running it for the first time). What are the training and testing accuracy? Copy the accuracies to the quiz and round to three decimals.         \n",
    "*Hint: The current progress is printed on the Jupyter Notebook terminal.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "We now want to compare an MLP without any hidden units with a single Perceptron. To do so, first train an MLP without an hidden layer by changing the given code.\n",
    "Copy your code to the quiz and check if it passes all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
